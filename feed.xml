<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://bonanzhu.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bonanzhu.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-25T08:16:37+00:00</updated><id>https://bonanzhu.com/feed.xml</id><title type="html">blank</title><subtitle>I am a Professor at School of Aerospace Engineering, Beijing Institute of Technology. Previously, I was a postdoctoral researcher at Department of Chemistry, University College London. I started as an experimental physicist in my undergraduate study in Cambridge, and gradually shifted into computational materials science/chemistry over the years. </subtitle><entry><title type="html">Access the internet (for good) on login nodes behind firewalls</title><link href="https://bonanzhu.com/blog/2022/internet-for-login-nodes/" rel="alternate" type="text/html" title="Access the internet (for good) on login nodes behind firewalls"/><published>2022-11-19T00:00:00+00:00</published><updated>2022-11-19T00:00:00+00:00</updated><id>https://bonanzhu.com/blog/2022/internet-for-login-nodes</id><content type="html" xml:base="https://bonanzhu.com/blog/2022/internet-for-login-nodes/"><![CDATA[<p>On some computing clusters the login nodes do not have internet access. This is a pain for installing software that requires pulling data from a repository, e.g. Python, Julia and many others. Lucky, there is a simple way to bypass this limitation without hacking through the firewall - SOCKS5 proxy with <code class="language-plaintext highlighter-rouge">ssh</code> <a href="https://man.openbsd.org/ssh">reverse tunnelling</a>. This allows the internet traffic to be diverted to a tunnel established by <code class="language-plaintext highlighter-rouge">ssh</code>, going through the client (local) machine.</p> <p>On the local machine, do:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh -R &lt;port_number&gt; &lt;username&gt;@&lt;hostname&gt;
</code></pre></div></div> <p>On the remote machine, one can set the following environmental variables to tell there is a SOCKS5 proxy available:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export http_proxy=socks5h://localhost:&lt;port_number&gt;
export https_proxy=socks5h://localhost:&lt;port_number&gt;
</code></pre></div></div> <p>or</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export http_proxy=socks5://localhost:&lt;port_number&gt;
export https_proxy=socks5://localhost:&lt;port_number&gt;
</code></pre></div></div> <p>Where <code class="language-plaintext highlighter-rouge">&lt;port_number&gt;</code> is just a port that is not in use. The only difference between the two is that the former will also divert domain resolution through SOCKS5. Note that certain programs (or different versions of the same program) may only support one (or none) of the above protocols. I have not extensively tested this, but seems that <code class="language-plaintext highlighter-rouge">wget</code> does not work with SOCKS5 at all, but <code class="language-plaintext highlighter-rouge">curl</code> does. For python <code class="language-plaintext highlighter-rouge">pip</code> should work (depends on the <code class="language-plaintext highlighter-rouge">requests</code> library), <code class="language-plaintext highlighter-rouge">conda</code> should also work, but it may need the environmental variable to be in the upper case… Note that for these two one can also specify the proxy directly in the command line or in the configuration file. For Julia, both the two ways above works with <code class="language-plaintext highlighter-rouge">Pkg</code> as of 1.8.</p>]]></content><author><name>Bonan Zhu</name></author><category term="posts"/><category term="posts"/><category term="misc"/><category term="linux"/><summary type="html"><![CDATA[On some computing clusters the login nodes do not have internet access. This is a pain for installing software that requires pulling data from a repository, e.g. Python, Julia and many others. Lucky, there is a simple way to bypass this limitation without hacking through the firewall - SOCKS5 proxy with ssh reverse tunnelling. This allows the internet traffic to be diverted to a tunnel established by ssh, going through the client (local) machine.]]></summary></entry><entry><title type="html">enabling data compression in AiiDA 2.0</title><link href="https://bonanzhu.com/blog/2022/aiida-2.0-and-compression/" rel="alternate" type="text/html" title="enabling data compression in AiiDA 2.0"/><published>2022-06-09T00:00:00+00:00</published><updated>2022-06-09T00:00:00+00:00</updated><id>https://bonanzhu.com/blog/2022/aiida-2.0-and-compression</id><content type="html" xml:base="https://bonanzhu.com/blog/2022/aiida-2.0-and-compression/"><![CDATA[<p>AiiDA 2.0 introduces the new storage format. Rather than placing files individually shard folders, an object storage is used. It includes storage backend stores the data and allows efficient retrival of the data using the hash computed from its content. By default, the <a href="https://github.com/aiidateam/disk-objectstore">disk-objectstore</a> is used for this. The filenames and folder structures are now stored inside the database instead.</p> <p>This backend stores writes in two ways. First, newly added files are stored as plain files on the disk, with the filename being its hash. This allows fully concurrent writing/reading. The storage can be <em>optimized</em> by concatenating these <em>loose</em> files into a single <em>packed</em> file, and the offsets and lengths for reading the data out is stored in a SQLite database. Optionally, when concatenating, the data stream may be compressed. Compression can lead to significant spacing saving for typical text output of DFT codes. While the <code class="language-plaintext highlighter-rouge">verdi storage maintain</code> command can be used to perform this, it is rather conservative and does not enable compression. To get around this, one can use <code class="language-plaintext highlighter-rouge">dostore optimize</code> to perform this task. Current, <code class="language-plaintext highlighter-rouge">disk-objectstore</code> does not support recompressing packed objects, so one may stuck with many uncompressed streams. In addition, when migrating from AiiDA v1, the migrated data is not compressed either. To force the compression, the following line should be changed in <code class="language-plaintext highlighter-rouge">aiida/storage/psql_dos/migrations/utils/utils.py</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">hashkeys</span> <span class="o">=</span> <span class="n">container</span><span class="p">.</span><span class="nf">add_streamed_objects_to_pack</span><span class="p">(</span><span class="n">streams</span><span class="p">,</span> <span class="n">compress</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">open_streams</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p>to</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">hashkeys</span> <span class="o">=</span> <span class="n">container</span><span class="p">.</span><span class="nf">add_streamed_objects_to_pack</span><span class="p">(</span><span class="n">streams</span><span class="p">,</span> <span class="n">compress</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">open_streams</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p>Do note that enabling compression would make the migration slower.</p>]]></content><author><name>Bonan Zhu</name></author><category term="posts"/><category term="posts"/><category term="aiida"/><summary type="html"><![CDATA[AiiDA 2.0 introduces the new storage format. Rather than placing files individually shard folders, an object storage is used. It includes storage backend stores the data and allows efficient retrival of the data using the hash computed from its content. By default, the disk-objectstore is used for this. The filenames and folder structures are now stored inside the database instead.]]></summary></entry><entry><title type="html">DFT parallelisation rule of thumb</title><link href="https://bonanzhu.com/blog/2022/DFT-parallelisation-rule-of-thumb/" rel="alternate" type="text/html" title="DFT parallelisation rule of thumb"/><published>2022-02-15T00:00:00+00:00</published><updated>2022-02-15T00:00:00+00:00</updated><id>https://bonanzhu.com/blog/2022/DFT-parallelisation-rule-of-thumb</id><content type="html" xml:base="https://bonanzhu.com/blog/2022/DFT-parallelisation-rule-of-thumb/"><![CDATA[<p><em>TL.DR for VASP</em></p> <ul> <li>Maximise k point parallelisation whenever possible</li> <li>keep number of bands per band group &gt; 10 for GGA calculation</li> <li>use as little lower plane wave parallelisation as possible for hybrid calculations, but keep bands per band group &gt; 4</li> <li>do not over parallelise by using too many core - you can end in the wrong side of the scaling curve</li> </ul> <p>When done efficiently plane wave density functional theory calculations can be very cost-effective for systems up to a few hundreds of atoms. However, there are things to watch out for and ones needs to understand the how parallelisation works in order to maximise the efficiency.</p> <p>While sensible “defaults” are often described in the documenting or chosen by the code itself, there is one thing that the code cannot choose - the number of MPI processors to be used. Unfortunately, when the calculation is slow, the common reaction is to put more CPUs at work, which, in some cases, can even leads to slower calculation and drastic performance drops.</p> <p>First, let’s think about basic theory of parallelisation. I will not pull out the exact equation here though. For any given program, it is consisted of parallisable and the serial parts. Adding more parallelism will make the former faster (in the ideal case), but leave the latter unchanged. This explains the inevitable drop of the speed-up vs num-of-cpus plot, as the latter becomes more and more the dominate contribution to the run time. In addition, parallelisation also have certain overheads, each processor needs to communicate with others in some ways, and the communication cost may increase with increase number of core. This is the cause of the drop in the efficiency with high core counts in the many case. Even if the communication cost does not scale with the number of cores, its contribution to the “parallelised” part still increases with the number of processors because of time of doing actual work becomes less and less.</p> <p>Second, let’s briefly words how plane wave DFT code are parallelised. Most of the codes are parallelised over MPI, the program that is ran with each MPI process, but directives are included in the code itself to orchestrate communication and memory distribution. The parallelisation is commonly conducted by three levels:</p> <ul> <li>kpoints</li> <li>bands</li> <li>plane wave coefficients</li> </ul> <p>Each level corresponds to one of the indices of the “wavefunction” ($W_{kbc}$) which is a multidimensional array. The kpoint parallelisation is the most efficient one, exploiting the fact that each kpoints are almost independent with each other - they only talk to each other via the electron density (which is not the case any more in hybrid DFT). The band parallelisation is conducted by distributing the bands over certain number of processors, and implementing parallel solvers and algorithms. The plane wave coefficients are involved in FFT, and they can be parallelised and distributed as well. Unlike the other two levels, parallelisation over kpoint does not distribute the memory, each process still receives the fully set of kpoints. This is not the case with band and plane wave parallelisation, however, these two does involve heavy (“all-to-all”) communications, making it more and more difficult to scale well on large core counts. On the other hand, one can expected almost perfect scaling with k-point paralellisation. There are of course many fine details on this topic, and many other part of the code (for example, auxiliary arrays and ionic solvers) can cost memory. The digrams below show the the relationship between these levels of parallelisation.</p> <pre><code class="language-mermaid!">flowchart 
subgraph All MPI Ranks

subgraph Kpoints
    KG1
    KG2[...]
end 

subgraph Bands
    B1
    B2[...]
end

subgraph G-vectors
        Proc1
        Proc2
        Proc3 
end
KG1 --&gt; Bands
B1 --&gt; G-vectors
end
</code></pre> <p>At bit on the terminology - the number of <em>kpoint/band groups</em> means that MPI processes are divided into $N$ groups for this level, and each group may contain $M$ number of processes. Hence, $M \times N$ is the total number of processes for this level. Typically, $N$ must be a divisor of the quantity to be parallelised over. For example, for a 320-process calculation with 4 kpoints and 120 bands, there can be:</p> <ul> <li>4 kpoint groups, each with 80 processes, each group works on 1 kpoint</li> <li>4 band groups inside each kpoint group, each group works on 30 bands</li> <li>inside each band group, the work is further parallelise over the G-vectors</li> </ul> <p>For VASP, this information can be obtained on the top of the <code class="language-plaintext highlighter-rouge">OUTCAR</code>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> running  384 mpi-ranks, with    1 threads/rank
 distrk:  each k-point on   32 cores,   12 groups
 distr:  one band on NCORE=   8 cores,    4 groups
</code></pre></div></div> <p>means there there are 12 kpoint groups (32 processes each), 4 band groups (8 processes each). This this calculation there are 48 band, hence each band group has 12 bands to work on. Further down the file:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> k-point  19 :   0.4000 0.4000 0.0000  plane waves:   21064
 k-point  20 :   0.5000 0.4000 0.0000  plane waves:   21070
 k-point  21 :   0.5000 0.5000 0.0000  plane waves:   21084

 maximum and minimum number of plane-waves per node :      2702     2616

</code></pre></div></div> <p>Shows that each processor inside the band group gets about 2700 plane-waves (G-vectors) to work on. If <code class="language-plaintext highlighter-rouge">NCORE</code> is increased, this value will reduce. When there are too few plane waves to be parallelised, the overall efficiency will drop sharply. However, increasing <code class="language-plaintext highlighter-rouge">NCORE</code> also increases the number of bands each band group works on, so improving the parallel efficiency over bands.</p> <p>To summarise, one should maximise the K-point parallelisation, and balance band and G-vector parallelisation by tuning the <code class="language-plaintext highlighter-rouge">NCORE</code> parameter. If one is in a regime that both band and G-vector parallelisation have similar efficiency, choosing <code class="language-plaintext highlighter-rouge">NCORE</code> roughly a square root of the number of processors per group would be optimum. For relatively small GGA calculations, setting <code class="language-plaintext highlighter-rouge">NCORE</code> to be equal to the number of cores in a <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">NUMA region</a> can be a sensible choice. In those cases, further optimisation of <code class="language-plaintext highlighter-rouge">NCORE</code> may only provide limited gain in speed.</p> <h2 id="hybrid-functional-calculations">Hybrid functional calculations</h2> <p>Hybrid functional calculations often requires a large amount of computing resources to be used for a single calculation to achieve reasonable time to solution (e.g. within a few days). While these calculations are easier to scale purely as a result of them being very compute-heavy (they scale to thousands of cores, while most GGA calculation won’t), careful tuning of parallelisation becomes even more important as every bit of performance gain can leads to huge saving of resources. Thus, one should carefully perform timing analysis for these calculations, typically using the <em>debug</em> or <em>short</em> queue of the supercomputer and run only for a few SCF cycles.</p> <p>In those cases, I would:</p> <ul> <li>parallelise over K points as much as possible if memory permits</li> <li>start from a low NCORE given that the number of bands per band group is more than 4</li> <li>to solve memory issue, reduce the kpoints before switching to “under-populate” the nodes.</li> </ul> <p>Because of the sheer among of resources needed for hybrid DFT, it is very easy to over-parallelise calculations and waste resources. So <em>test, test, test</em>…..</p> <p>Example tests over <code class="language-plaintext highlighter-rouge">NCORE</code> for a 65-atom CdTe defect supercell calculation with 8 kpoints (<code class="language-plaintext highlighter-rouge">KPAR=8</code>), 346 bands (minimum), 47528 plane waves using HSE06 (<code class="language-plaintext highlighter-rouge">ALGO=normal</code>) with VASP6 on <a href="https://www.archer2.ac.uk">ARCHER2</a>:</p> <p><img src="/assets/img/CdTe_Int.png" alt="CdTe defect calculation"/></p> <p>It can be seen that the best performance is achieved with <code class="language-plaintext highlighter-rouge">NCORE=4</code> note that this calculation may actually involve more bands than <code class="language-plaintext highlighter-rouge">NCORE=8</code> and <code class="language-plaintext highlighter-rouge">NCORE=16</code>. Using <code class="language-plaintext highlighter-rouge">NCORE=1</code> or <code class="language-plaintext highlighter-rouge">NCORE=2</code> resulted in very bad performance, probably because there were only one or two bands per band group, and note that the speed up from 1280 to 2560 cores is even worse. <code class="language-plaintext highlighter-rouge">NCORE=4</code> result in about 5 bands per band group (of 80 cores) for the 2560-core calculation and 9 bands per group (of 40 cores) for the 1280-core calculation. Also note the speed up by doubling the core-count - we achieve a 80% gain in speed by throwing twice the computing resource - not bad at all.</p> <h2 id="further-topics">Further topics</h2> <p>We haven not touched OpenMP parallelisation and GPU parallelisation. I have seen good performance with the latter on latest generation of GPUs (e.g. Nvidia A100) for compute heavy hybrid functional VASP calculations. The OpenMP parallelisation can often be used to “recycle” idle cores on underpopulated nodes.</p> <p>We have not touch parallelisation in CASTEP, which is often planed by the code itself rather than the user. Nevertheless, manual tuning can be valuable for large-scale calculations to achieve the good performance.</p>]]></content><author><name>Bonan Zhu</name></author><category term="posts"/><category term="posts"/><summary type="html"><![CDATA[TL.DR for VASP]]></summary></entry><entry><title type="html">Parallel efficiencies of (small) plane wave DFT calculations</title><link href="https://bonanzhu.com/blog/2021/parallel-efficiency-of-pw-DFT/" rel="alternate" type="text/html" title="Parallel efficiencies of (small) plane wave DFT calculations"/><published>2021-07-14T00:00:00+00:00</published><updated>2021-07-14T00:00:00+00:00</updated><id>https://bonanzhu.com/blog/2021/parallel-efficiency-of-pw-DFT</id><content type="html" xml:base="https://bonanzhu.com/blog/2021/parallel-efficiency-of-pw-DFT/"><![CDATA[<p>Plane wave DFT calculations are often known as “cubic-scaling” where the cost grows as the number of atoms (more precisely the number of electrons) cubed. Thankfully, they can be parallelised over many (possibly very large number of cores) to accelerate the calculations. This is often done at multiple levels: the plane wave coefficients, the bands and the k points.</p> <p>A frequent question one may ask when running calculations is: how many cores should I use? Obviously, using more cores <em>should</em> make the calculation faster, but it also increases the time spend on inter-process communications, causing the parallel efficiency to drop with increasing core counts. It not uncommon to see examples where a single calculation can be parallelised over thousands of cores for a supercomputer with relatively small drop on the efficiency. However, the rate of reduction in the parallel efficiency is also highly dependent on the size of the system: those of hard problems (e.g. more atoms) drop much slower than simple and small systems.</p> <p>If there are many calculations to run through, rather than getting the result of each calculation quickly, it can be more efficient to run multiple calculations in parallel, each using a smaller number cores to achieve a higher throughput.</p> <p>Below are some test results for a 28-atom structure using the code <a href="">CASTEP</a> with increasing number of cores while maintaining a full population of single compute node with 24 cores. This is a “small” calculation with only 340 eV plane wave cut off energy and 4 kpoints, and 106 bands.</p> <p><img src="/assets/img/throughput-castep-full-occ.png" alt="Test result"/></p> <p>As one may expect, the parallelisation efficiency drops with increasing number of cores. By running 24 cores jobs instead of 1 cores, the overall throughput has dropped to about 80 %, while running six 4-core jobs has only a 5% lost in efficiency. While running many “small” jobs does increase the overall throughput, it comes at the cost of very long “time-to-result” for each calculation. In reality, computing clusters often have cap of run times, and having long-running time for each calculation risks having unfinished ones getting killed, wasting a certain amount of time and resources. This also means that the aftermath of having any kind of node failure will be higher, as calculations have longer turnaround.</p> <p>One important note is that if the node is not fully occupied in the tests, the benefit of running smaller but and many jobs can be significantly exaggerated, as shown in the plot below.</p> <p><img src="/assets/img/throughput-castep-partial-occ.png" alt="Test result with partial occupation"/></p> <p>Several factors could cause this. First, under populating the node means each MPI process has access to more memory and communication bandwidth. Second, modern CPUs often <a href="https://en.wikipedia.org/wiki/Intel_Turbo_Boost">boosts the frequencies</a> when only a few cores are active to be able to utilise the thermal headroom left by the idling ones.</p>]]></content><author><name>Bonan Zhu</name></author><category term="posts"/><category term="DFT"/><category term="misc"/><category term="posts"/><summary type="html"><![CDATA[Plane wave DFT calculations are often known as “cubic-scaling” where the cost grows as the number of atoms (more precisely the number of electrons) cubed. Thankfully, they can be parallelised over many (possibly very large number of cores) to accelerate the calculations. This is often done at multiple levels: the plane wave coefficients, the bands and the k points.]]></summary></entry><entry><title type="html">Hello World!</title><link href="https://bonanzhu.com/blog/2021/hello-world/" rel="alternate" type="text/html" title="Hello World!"/><published>2021-07-13T00:00:00+00:00</published><updated>2021-07-13T00:00:00+00:00</updated><id>https://bonanzhu.com/blog/2021/hello-world</id><content type="html" xml:base="https://bonanzhu.com/blog/2021/hello-world/"><![CDATA[<p>This my first post using Jekyll - so hello world. The aim of this site is to provide a peronal space for sharing my research and also act as a blog. From time to time, I will post contents related to computational materials science and programming. Hopefully, they will be of the interests of my fellow researchers and peers!</p>]]></content><author><name>Bonan Zhu</name></author><category term="posts"/><category term="posts"/><category term="misc"/><summary type="html"><![CDATA[This my first post using Jekyll - so hello world. The aim of this site is to provide a peronal space for sharing my research and also act as a blog. From time to time, I will post contents related to computational materials science and programming. Hopefully, they will be of the interests of my fellow researchers and peers!]]></summary></entry></feed>